---
title: 'Building a movie recommendation system using the MovieLens (10MM) dataset'
subtitle: 'HarvardX Data Science Professional Certificate'
output: 
  pdf_document:
    latex_engine: xelatex
header-includes:
  - \usepackage{amssymb}
date: "`r format(Sys.Date(), '%d %B, %Y')`"
---

```{r setup, include = TRUE, echo = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

```{r data_sets, eval = TRUE}

##########################################################
# Create edx and final_holdout_test sets 
##########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip
#options(timeout = 900)

dl <- "ml-10M100K.zip"
if(!file.exists(dl))
  download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings_file <- "ml-10M100K/ratings.dat"
if(!file.exists(ratings_file))
  unzip(dl, ratings_file)

movies_file <- "ml-10M100K/movies.dat"
if(!file.exists(movies_file))
  unzip(dl, movies_file)

ratings <- as.data.frame(str_split(read_lines(ratings_file), fixed("::"), simplify = TRUE),
                         stringsAsFactors = FALSE)
colnames(ratings) <- c("userId", "movieId", "rating", "timestamp")

ratings <- ratings |> 
  mutate(userId = as.integer(userId),
         movieId = as.integer(movieId),
         rating = as.numeric(rating),
         timestamp = as.integer(timestamp))

movies <- as.data.frame(str_split(read_lines(movies_file), fixed("::"), simplify = TRUE),
                        stringsAsFactors = FALSE)
colnames(movies) <- c("movieId", "title", "genres")

movies <- movies |> 
  mutate(movieId = as.integer(movieId))

movielens <- left_join(ratings, movies, by = "movieId")

# Final hold-out test set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.6 or later

test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx  <- movielens[-test_index,]
temp <- movielens[ test_index,]

# Make sure userId and movieId in final hold-out test set are also in edx set
final_holdout_test <- temp |> 
  semi_join(edx, by = 'movieId') |> 
  semi_join(edx, by = 'userId')

# Add rows removed from final hold-out test set back into edx set
removed <- anti_join(temp, final_holdout_test)
edx <- rbind(edx, removed)
```

```{r theme_setting}
# Setting the theme for ggplot figures
theme_set(theme_minimal() + 
            theme(legend.position = 'bottom',
                  panel.grid = element_blank(),
                  text = element_text(family = 'serif'),
                  panel.border = element_rect(color = 'grey66', 
                                               fill = NA, 
                                               size = .5))) 
```

```{r additional_libraries}
if(!require(data.table)) install.packages('data.table', repos = 'http://cran.us.r-project.org')
if(!require(ggrepel)) install.packages('ggrepel', repos = 'http://cran.us.r-project.org')
if(!require(kableExtra)) install.packages('kableExtra', repos = 'http://cran.us.r-project.org')
if(!require(tictoc)) install.packages('tictoc', repos = 'http://cran.us.r-project.org')
if(!require(doParallel)) install.packages('doParallel', repos = 'http://cran.us.r-project.org')
if(!require(rsample)) install.packages('kableExtra', repos = 'http://cran.us.r-project.org')
if(!require(ggridges)) install.packages('ggridges', repos = 'http://cran.us.r-project.org')
```

```{r Feature_transformation}
# Transform some features in both the edx and holdout sets
# Training set
edx <- edx |> 
  mutate(
    year_released = as.integer(str_extract(title, '(?<=[(])\\d{4}(?=[)])')),
    Date = as_datetime(timestamp), 
    year_reviewed = year(Date), 
       diff_years = year_reviewed - year_released,
           detect = str_detect(genres, '\\|'),
            single_genre = ifelse(detect, str_extract(genres, '^[^|]+'), genres),
      half_rating = str_detect(rating, '\\.'),
               AM = am(Date),
         date_month = floor_date(Date, 'month'),
    y_day = yday(Date))

# Final_holdout_test set
final_holdout_test <- final_holdout_test |> 
  mutate(
    year_released = as.integer(str_extract(title, '(?<=[(])\\d{4}(?=[)])')),
             Date = as_datetime(timestamp),
    year_reviewed = year(Date), 
       diff_years = year_reviewed - year_released,
           detect = str_detect(genres, '\\|'),
            single_genre = ifelse(detect, str_extract(genres, '^[^|]+'), genres),
      half_rating = str_detect(rating, '\\.'),
               AM = am(Date),
         date_month = floor_date(Date, 'month'),
    y_day = yday(Date))
```

*Summary: In this project, I develop a movie recommendation system using the MovieLens dataset. The approach used utilizes biases associated with movies, users, genres, and years to address the challenge of excessive predictors in regression analysis, aiming to strike a balance between complexity and predictive accuracy through regularization. However, signs of underfitting are observed due to the model's simplicity. Exploring alternative methodologies to tackle computational constraints could enhance the model's effectiveness, potentially involving distributed computing frameworks or algorithmic optimizations. Despite limitations, this project showcases an innovative approach to predictive modeling, integrating biases and regularization techniques to overcome regression analysis challenges. Future enhancements may focus on refining computational efficiency and feature engineering methodologies to bolster the model's performance.*

# I. Introduction

## Objective

In this project I develop a movie recommendation system using the ten-million-row version of the MovieLens dataset. This dataset was  issued and is actively maintained by GroupLens Research, a research group at the University of Minnesota.

Movie recommendation systems analyze movie preferences of users, typically revealed through their ratings. By examining these ratings, the system identifies patterns and similarities among users' tastes and preferences. Leveraging this information, it then identifies similarities between the preferences of different users for movies. This process enables the system to make recommendations for movies that align with the interests of a potential viewer.

The ultimate  objective of this project is to develop a recommendation system with a loss function, specifically defined as the root mean squared error (RMSE), where error is defined as the difference between the actual and the estimated rating. More specifically, RMSE can be expressed as follows:

 $$ \text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2} $$ where \( n \) is the number of ratings, \( y_i \) is the actual rating, and \( \hat{y}_i \) is the predicted value.

## The **movielens** datset

The **movielens** version used in this project contains 10,000,054 ratings from 69,878 unique individuals, rating 10,677 distinct movies. This version, named edx, was partitioned to create a holdout set containing 10 percent of the data. As will be explained later, the remaining 90% was further partitioned for validation purposes.

Table 1 below shows details of the variables in the dataset, including brief explanations and their respective classes.

```{r Table_1}
# Create a table of the variables in the dataset
classes <- map_chr(movielens, ~ class(.))
names <- names(movielens)

`Brief explanation` <- c(
  'Unique identifier assigned to each user',
  'Unique identifier assigned to each movie',
  'Valuation given by users to movies',
  'Seconds since Jan. 1, 1970 to the moment of review',
  'Title of movie, including its year of release in parentheses',
  'Categories that group films by some measure of similarity')

 tibble( Name = names, 
        Class = classes, 
        `Brief explanation`,
    `Used as` = c(rep('Feature', 2), 'Target', rep('Feature', 3)),
           id = c(3, 1, 2, 6, 4, 5)) |> 
   arrange(id) |> 
   select(1:4) |> 
   kbl(booktabs = TRUE, 
    linesep = '',
    caption = 'Variables in the movielens dataset') |> 
  kable_styling(latex_options = c("HOLD_position"))
```

In Table 1:

- *rating* represents the subjective valuation provided by users to each movie they rate. This feature, ranging from 0.5 to 5 in 0.5 increments, is the target variable, that is, the variable that we want to predict in this project.

- *usedId* is a unique identifier for each of the users providing their movie ratings.

- *movieId* and *title* refer to individual movies. There is a one-to-one relationship between *movieId*, which is a unique identifier for each movie, and *title*, which refers to the title of movies, including their year of release in parentheses.^[While *movieId* typically serves as a unique identifier, there is a case where two different identifiers are assigned to a single *title*. Unfortunately, with the available information, it is not possible to determine whether there are two titles and one is missing or there is a single title that was assigned two different identifiers. However, considering the size of our training data and our chosen modeling approach (please refer to Section II for our modeling approach), this ambiguity is not expected to introduce any systematic errors in our results].

- *genres* categorizes films based on shared themes, styles, and narrative elements. In its original categorization, films are sorted into either single genres or combinations of genres, some of which may overlap. For example, Movie A might belong to a single category like "Comedy," while Movie B could be categorized as "Comedy, Drama, and Romantic." To simplify this categorization, we introduce a transformed feature called *single_genre*. This feature retains either the single genre listed in the original classification or, in cases of multiple genres, the first one listed, assuming it best represents the movie. With this simplified categorization, the number of genres reduces from 797 to 20, thus facilitating simpler graphical representations (see Figure 1 below). However, it is important to note that in modeling the target variable, we retain the original classification, *genres*, as it allows for a finer level of classification and, therefore, more variability and, presumably, less bias.

```{r Figure_1}
# Figure 1 - 
fig_1 <- edx |> 
  select(genres, single_genre) |>
  filter(single_genre != '(no genres listed)' |
         single_genre != 'IMAX' |
         single_genre != 'War') |> 
  ggplot(aes(genres, single_genre)) +
  geom_count(alpha = 1/3) +
  theme(axis.title = element_blank(),
        axis.ticks = element_blank(),
  panel.grid.minor = element_line(color = 'grey89',
                   linewidth = .25)) 
fig_1 +
  labs(title = 'Figure 1: Mapping of 797 movie genres into 20 broad categories',
       x = '797 original categories',
       y = '20 categories') +
  theme(axis.text.x = element_blank())
```

- *timestamp* is a numerical value representing the specific moment in time when a rating was submitted, measured in seconds elapsed since the start of January 1, 1970. ^[Also known as the Unix epoch.] This standardized time format allows for precise recording and comparison of when ratings were provided, aiding in a temporal dimension to our analysis.

\newpage

# II. Methods 

This section provides an overview of the main characteristics of the target variable, *rating*, and the features utilized for its prediction. Our data exploration primarily involves visualizing univariate and bivariate relationships between the outcome and features. It is worth noting that not all insights presented in this section will be incorporated into our modeling approach. The main reason being that although many of these findings could have been integrated using regression analysis, regression was not a feasible option.

## Data exploration and visualization

- Average movie ratings and sample size

In Figure 2 below, each data point represents a combination of average rating and number of ratings for each individual movie. The blue line indicates the expected rating for different levels of review counts. As the number of reviews increases along the horizontal axis, the average rating rises and, as expected, its dispersion becomes narrower. ^[The noted inverse relationship between average rating and dispersion is a statistical regularity that arises because, in our context, the sampling distribution of the average rating for a particular movie is inversely related to the square root of the number of reviews it receives. For a clear and engaging explanation of this regularity see Wainer (2007).]

```{r Figure_2, warning = FALSE}
# Figure 2 - average rating per movie as a function of number of ratings
fig_2 <- edx |> 
  select(movieId, rating, single_genre) |>
  group_by(movieId, single_genre) |> 
  reframe(total = n(), 
     avg_rating = mean(rating)) |> 
  distinct() |> 
  ggplot(aes(total/1e3, avg_rating)) +
  geom_point(size = 1/5, alpha = 1/3) + 
  geom_smooth(se = F,  linewidth = 1/3, method = 'lm') +
  facet_wrap(~single_genre)
fig_2 +
  labs(title = 'Figure 2: Average rating vs. number of ratings',
           x = 'Sample size (in thousands)', 
           y = 'Average rating')
```

- Count of ratings by user

Table 2 below presents summary statistics of the distribution of the total number of ratings by individual users. This distribution is rightly skewed (since the mean is about twice the median), and ranges from 10 to an astonishing 6,616. 

```{r Ratings_by_users, eval = TRUE}
# Count of ratings by user
ratings_userId <- edx |> 
  count(userId)
# Summary
val <- summary(ratings_userId$n) |> 
  tibble() |> 
  pull()
  Summary <- tibble(
    Statistic = c(
    'Minimum', 
    'Q1', 
    'Median', 
    'Mean',
    'Q3',
    'Maximum'), 
    Value = round(val))

kbl(Summary, booktabs = TRUE, 
    digits = 4, 
    linesep = '', 
    caption = 'Review count per user') |> 
  kable_styling(latex_options = c("HOLD_position"))
```

Table 3 presents cumulative values of reported number of ratings by user spanning from 1995 to 2009, which can be challenging to interpret. To provide a clearer perspective on these numbers, Figure 3 below illustrates the relationship between the number of reviews per day and average ratings.

```{r Table_daily_count}
# Table: Users with atypical (excessively high) daily rating counts' 
super_raters <- edx |> 
  group_by(userId, year_reviewed) |> 
  reframe(Yearly = n(),
          Daily = Yearly / 360) |> 
  arrange(desc(Daily)) |> 
  filter(Daily > 6) 

super_raters |> 
  kbl(booktabs = TRUE, 
    digits = 0, 
    linesep = '',
    caption = 'Users with atypical (excessively high) daily rating counts') |> 
  kable_styling(latex_options = c('HOLD_position'))
```

In this figure, all dots corresponding to users who reported reviewing approximately at least 7 movies per day are annotated by their  *userId*. For example, the two rightmost values in this figure indicate that these users reviewed 10 and 13 movies each day on average, respectively. Given that the average duration of movies falls between 90 and 120 minutes, these users may not necessarily have spent between 900 minutes (or 15 hours) and 1200 minutes (20 hours) per day watching movies. This is because *...Ratings in MovieLens can occur at any time, possibly many years after watching a movie. Users often enter a large number of ratings in a single session, either to fill in their rating history for personal satisfaction or in hopes of receiving more personalized recommendations.* (Maxwell and Constant, 2015, p. 15). In any case, it should be clear that these extreme reports put into question the reliability of their corresponding ratings.

```{r Figure_3}
# Figure 3: Daily rating count
by_userId <- edx |> 
  select(userId, year_reviewed, rating) |> 
group_by(userId, year_reviewed) |> 
reframe(Mean  = mean(rating),
            N = n() / 360) # Ignoring leap years ()
fig_3 <- by_userId |> 
  ggplot(aes(N, Mean, label = userId)) +
  geom_count(alpha = .25) +
  geom_text_repel(data = subset(by_userId, N > 6), 
                  size = 2.5, 
                  max.overlaps = Inf, 
                  min.segment.length = 0,
                  segment.size = .12) +
  scale_x_continuous(breaks = c(1, 4, 7, 10, 13), labels = c('1', '4', '7', '10', '13')) 
fig_3 +
  labs(title = 'Figure 3: Daily movie rating count',
          x = 'Number or ratings',
          y = 'Average rating') + 
  theme(legend.position = 'none')
```    

- Users giving identical ratings regardless of movie

```{r invariant_ratings, eval = T}
# for Table 4 -Invariant ratings
invariant_ratings <- edx |> 
  group_by(userId) |> 
  reframe(min = min(rating),
          max = max(rating),
          N = n(),
          test = max / min) |> 
  filter(test == 1)
```

Similarly, `r nrow(invariant_ratings)` users consistently gave the same ratings to every movie they reviewed. This type of review, void of critical assessment, overlooks the uniqueness and individual characteristics of each film. Consequently, it compromises the validity of these ratings. Table 4 below shows these cases.  

```{r Table, eval = T}
# Table 4
  invariant_ratings |> 
  select(1, 2, 4) |> 
  arrange(desc(N)) |> 
  kbl(booktabs = TRUE,
     col.names = c('userId', 'Rating', 'Movies reviewed'),
        digits = c(0, 2, 0),
       linesep = '',
       caption = 'Users giving identical ratings regardless of movie') |> 
  kable_styling(latex_options = c('HOLD_position'))
```

- Movies

```{r Distinct_movies, eval = T}
# In the edx data set, there are 10677 distinct movies.
distinct_movieId <- distinct(edx, movieId) |> 
  nrow()

# However, there are only 10676 distinct titles
distinct_title <-  distinct(edx, title) |> 
nrow()
```

There are `r distinct_movieId` movies in the edx dataset but only `r distinct_title` distinct titles. This is because the movie *War of the Worlds (2005)* has two different Ids. 

```{r war_of_the_worlds, eval = T}
# War_of_the_worlds
edx |> 
  filter(title == 'War of the Worlds (2005)') |>
  group_by(movieId) |> 
  reframe(title = title,
          n = n()) |> 
  distinct() |> 
  kbl(booktabs = TRUE, 
    linesep = '',
    caption = 'War of the Worlds (2005)') |> 
  kable_styling(latex_options = c("HOLD_position"))
```

- Average rating by released year

Figure 4 below shows the average rating of movies over time. The dot sizes are proportional to the number of ratings used to estimate these averages. The color represents variability, measured by the standard deviation of ratings. This figure suggests that since the 1970s, average ratings have been lower, though they are based on a significantly larger number of ratings.
 
```{r Figure_4, eval = T}
by_year_released <- edx |> 
group_by(year_released) |> 
reframe(Mean = mean(rating),
           N = n(), 
          SD = sd(rating))
           
fig_4 <- by_year_released |> 
     ggplot(aes(year_released, Mean, 
                size = N, 
                color = SD)) +
     geom_point(alpha = .5) +
  geom_smooth(span = 1.5, se = FALSE, linewidth = .25)
     
  fig_4 +
     labs(title = 'Figure 4: Decline in average movie ratings despite increased ratings volume \nsince the 1970s', 
          x = 'Year movie was released',
          y = 'Average rating')
```

```{r, eval = T}
# Exclude some genres with just a few movies 
# (this has no effect on the final_holdout_test, 
# since the associated movies are only in the edx)
edx <- edx |> 
  filter(single_genre != '(no genres listed)' |
         single_genre != 'IMAX'               |
         single_genre != 'War')
```

## Modeling approach

The prediction of numerical outcomes, such as average movie ratings, is commonly achieved through regression analysis. However, due to the large number of categories in certain features, notably the *genres* category, estimating a regression model with the available computational resources is practically unfeasible. 

```{r}
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings) ^ 2, na.rm = TRUE)) 
}
```

Under these circumstances, traditional regression models falter due to the exponential increase in computational complexity, rendering them impractical for analysis. To surmount this issue, I basically follow the approach presented in Irizarry (2020, Chapter 33.7).

Let's define the simplest representation of movie ratings as:

\[ Y_{u,i} = \mu + \epsilon_{u,i} \]

where, 

- \( Y_{u,i} \), denote the observed rating for user \( u \) and movie \( i \);
- \( \mu \), denote the overall average rating across all users and items; and
- \( \epsilon_{u,i} \), is an error term associated with the rating for user \( u \) and item \( i \). This term captures the discrepancy between the observed rating and the predicted rating from the model.

### Adding movie effects

In what follows, for simplicity, I am ignoring the error term, \( \epsilon_{u,i} \).

\[ Y_{u,i} = \mu + b_{1,i} + \epsilon_{u,i} \]

One of the pivotal components of the model is the movie-specific bias \( b_{1,i} \), which encapsulates the inherent characteristics of each movie in the dataset. By calculating this bias through a weighted aggregation of the deviations between actual ratings \( Y_{u,i} \) and the global average rating (\( \mu \)) across all users who have rated the movie, the model can estimate the idiosyncrasies of individual movies: 

 \[b_{1,i} = Y_{u,i} - \mu \] 
 
### Adding user effects

\[ Y_{u,i} = \mu + b_{1,i} + b_{2,u} + \epsilon_{u,i} \]

Similarly, the user-specific bias \( b_{2,u} \) enriches the model's predictive capabilities by capturing the unique rating behaviors exhibited by individual users. This bias delineates the discrepancy between a user's ratings, the global average rating (\( \mu \)), and the movie-specific bias. In essence, the user specific bias serves as a corrective mechanism, reducing the influence of individual users on the overall rating prediction. It is express as:

\[ b_{2,u} = Y_{u,i} - \mu - b_{1,i}\]

Finally, the model extends its predictive prowess by incorporating genre-specific biases (\( b_{3,g(i)} \)) and year-specific biases (\( b_y \)), each playing a crucial role in capturing genre and temporal trends in user preferences, respectively:

### Adding genre effects

\[ Y_{u,i} = \mu + b_{1,i} + b_{2,u} + b_{3,g(i)} + \epsilon_{u,i} \]

### Adding temporal effects

\[ Y_{u,i} = \mu + b_{1,i} + b_{2,u} + b_{3,g(i)} + b_yf(t_{u,i}) + \epsilon_{u,i} \]

For the sake of brevity, I will omit the detailed derivation of these effects. However, they can be easily obtained by following the same approach used previously for calculating the earlier effects.

# III. Results

The first step in modeling movie ratings involves creating a new training set (see code below) from the edx dataset to avoid data leakage and ensure robust model evaluation. Data leakage occurs when information from the test set inadvertently influences the training process, leading to overly optimistic performance estimates. To mitigate this risk, a new training set is constructed by excluding movies and users that appear in the final holdout and test sets. This ensures that the model is trained only on information that would realistically be available at the time of prediction, enhancing its ability to generalize to unseen data.

```{r training_set, echo = T}
# Creating a training set 
set.seed(1, sample.kind = 'Rounding')
test_index <-  createDataPartition(y = edx$rating, times = 1, p = 0.1, list = F)
training_set <-  edx[-test_index,]
temp <-  edx[test_index,]
# Make sure userId and movieId in edx_test set are also in training_set
edx_test <-  temp |> 
semi_join(training_set, by = 'movieId') |> 
semi_join(training_set, by = 'userId') |>  
as.data.table()
# Add rows removed from edx_test set back into training_set
removed_new <-  anti_join(temp, edx_test)
training_set <-  rbind(training_set, removed_new) |>  
  as.data.table() # for faster computation
rm(removed_new, temp, test_index)
```

This separation of data allows for a more accurate assessment of the model's performance on unseen data. Additionally, by incorporating a validation set for hyperparameter tuning and a separate test set for final evaluation, the integrity of the modeling process is maintained, enabling reliable predictions.

```{r biases, eval = T, echo = F}
# Global average rating
mu <- mean(training_set$rating, na.rm = T)
#RMSE 
mu_rmse <- RMSE(mu, training_set$rating)

# Movie bias
b_movie <- training_set |> 
  group_by(movieId) |> 
  summarize(b_movie = mean(rating - mu))

# User bias
b_user <- training_set |> 
  left_join(b_movie, by = 'movieId') |> 
  group_by(userId) |> 
  summarize(b_user = mean(rating - mu - b_movie))

# Movie genre bias
b_genres <- training_set |> 
  left_join(b_movie, by = 'movieId') |> 
  left_join(b_user,  by = 'userId') |> 
  group_by(genres) |>
  summarize(b_genres = mean(rating - mu - b_movie - b_user))

# Year bias
b_year <- training_set |> 
  left_join(b_movie,   by = 'movieId') |>
  left_join(b_user,    by = 'userId') |>
  left_join(b_genres,  by = 'genres') |>
  group_by(year_reviewed) |> 
  summarize(b_year = mean(rating - mu - b_movie - b_user - b_genres))
# Adding year_reviewed to final_holdout_test
final_holdout_test <- final_holdout_test |> 
  mutate(year_reviewed = year(as_datetime(timestamp)))

# Predicted ratings movie
predicted_ratings_movie <- final_holdout_test |> 
  left_join(b_movie,  by = 'movieId') |> 
mutate(pred = mu + b_movie) 

# Predicted ratings user
predicted_ratings_user <- final_holdout_test |> 
  left_join(b_movie,  by = 'movieId') |> 
  left_join(b_user,   by = 'userId') |> 
mutate(pred = mu + b_movie + b_user) 

# Predicted ratings genres
predicted_ratings_genres <- final_holdout_test |> 
  left_join(b_movie,  by = 'movieId') |> 
  left_join(b_user,   by = 'userId') |> 
  left_join(b_genres, by = 'genres') |> 
mutate(pred = mu + b_movie + b_user + b_genres) 

# Predicted ratings year
predicted_ratings_year <- final_holdout_test |> 
  left_join(b_movie,  by = 'movieId') |> 
  left_join(b_user,   by = 'userId') |> 
  left_join(b_genres, by = 'genres') |> 
  left_join(b_year,   by = 'year_reviewed') |> 
mutate(pred = mu + b_movie + b_user + b_genres + b_year) 

RMSE_movie <- RMSE(final_holdout_test$rating, predicted_ratings_movie$pred)
RMSE_user <- RMSE(final_holdout_test$rating, predicted_ratings_user$pred)
RMSE_genres <- RMSE(final_holdout_test$rating, predicted_ratings_genres$pred)
RMSE_year <- RMSE(final_holdout_test$rating, predicted_ratings_year$pred)
```

Table 6 below presents the performance of different models in predicting ratings, each model accounting for increasingly complex factors. The first model, relying solely on the global average rating, yields an RMSE of `r mu_rmse`, indicating the error in predictions made using only this simple metric. As the models become more complex, incorporating factors such as movie effects, user effects, genre effects, and year effects, the RMSE values steadily decrease. For instance, including movie effects alone reduces the RMSE to `r RMSE_movie`, suggesting a notable improvement in prediction accuracy compared to the simplistic global average approach. Subsequent models incorporating user, genre, and year effects show further reductions in RMSE, indicating enhanced predictive power as more nuanced factors are considered. Overall, this progression highlights the importance of accounting for various factors in accurately predicting ratings, with each additional factor contributing to a more precise model.

```{r rmse}

interm_table <- tribble(~Model, ~RMSE,
        'Simple, global average', mu_rmse,
        'Movie effects', RMSE_movie,
        'Movie and user effects', RMSE_user,
        'Movie, user and genre effects', RMSE_genres,
        'Movie, user, genre and year effects', RMSE_year)

interm_table |> 
  kbl(format = 'latex', booktabs = TRUE, escape = FALSE, 
    linesep = '', 
    caption = 'RMSE') |>
  kable_styling(latex_options = c('hold_position'))
```

However, despite the improvements observed, further enhancements in prediction accuracy can be achieved through the use of regularization techniques. Regularization is a method for preventing overfitting and improving the generalization ability of models by penalizing overly complex parameter estimates. By adding a regularization term to the loss function, models favor simpler solutions, effectively balancing between fitting the training data well and avoiding excessive complexity.

One common regularization technique used in linear models, including collaborative filtering models, is ridge regression, also known as $L_2$ regularization. Ridge regression adds a penalty term proportional to the square of the magnitude of the coefficients, thereby shrinking their values towards zero without eliminating them entirely. This helps to mitigate the problem of multicollinearity and reduces the risk of overfitting by imposing constraints on the model's parameter estimates.

In the context of collaborative filtering, regularization is particularly valuable in preventing the model from learning overly specific patterns from the training data that may not generalize well to unseen data. 

Optimizing the regularization strength, typically controlled by a hyperparameter $\lambda$, is crucial for achieving optimal model performance. The choice of $\lambda$ involves a trade-off between model simplicity and predictive accuracy. A small $\lambda$ allows for more flexibility in the model but may lead to overfitting, while a large  $\lambda$ imposes stronger regularization, potentially resulting in underfitting. In what follows $\lambda$ will be tuned through cross-validation or grid search is essential to find the optimal balance and maximize the model's predictive power.

## Tuning lambda

The code below conducts cross-validation to tune the regularization parameter $\lambda$, which controls the strength of regularization. It defines a function, `RMSEs`, to calculate RMSE using regularization with $\lambda$. This function computes biases for movies, users, genres, and years, incorporates these biases into predictions, and returns the RMSE for a given $\lambda$. 

Next, the code sets up cross-validation with 5 folds and specifies a sequence of $\lambda$ values to evaluate. Within a nested loop, it iterates over each $\lambda$ value and each fold, extracting training and test data for each fold and calculating the RMSE using the `RMSEs` function. After completing cross-validation, the code calculates the mean RMSE for each $\lambda$ and identifies the optimal $\lambda$ value that minimizes the mean RMSE.

```{r tunning_lambda, echo = T}
# Function for tuning lambda
set.seed(1)

RMSEs <- function(train_data, test_data, lambda) {
  mu <- mean(train_data$rating)
  
# Convert train_data to data.table for faster computations
  train_dt <- as.data.table(train_data)
  
# Movie bias 
  b_i <- train_dt[, .(b_i = sum(rating - mu) / (.N + lambda)), 
                  by = movieId]
# User bias  
  b_u <- merge(train_dt, b_i[, .(movieId, b_i)], 
               by = "movieId", all.x = TRUE)[,
               .(b_u = sum(rating - b_i - mu) / (.N + lambda)), by = userId]
# Gender bias  
  b_g <- merge(merge(train_dt, b_i, by = "movieId", all.x = TRUE),
               b_u, by = "userId", all.x = TRUE)[,
               .(b_g = sum(rating - mu - b_i - b_u) / (.N + lambda)), 
               by = genres]
  
  # Year bias (year_released)
  b_y <- merge(merge(merge(train_dt, b_i, by = "movieId", all.x = TRUE),
              b_u, by = "userId", all.x = TRUE),
              b_g, by = "genres", all.x = TRUE)[,
              .(b_y = sum(rating - mu - b_i - b_u - b_g) / (.N + lambda)), 
              by = year_released]
  
# Convert test_data to data.table for faster join
  test_dt <- as.data.table(test_data)
  
  predicted_ratings <- merge(
    merge(merge(merge(test_dt, b_i, by = 'movieId', all.x = TRUE),
             b_u, by = 'userId', all.x = TRUE),
             b_g, by = 'genres', all.x = TRUE),
             b_y, by = 'year_released', all.x = TRUE)[,
               pred := mu + b_i + b_u + b_g + b_y][!is.na(pred)]
  
return(RMSE(predicted_ratings$pred, predicted_ratings$rating))
}

n_folds <- 5 # 2 folds 72.89 secs; 5 folds: 204.327 secs; 10 folds: 435.717; 20 folds: a lot

# Create folds
  folds <- vfold_cv(training_set, v = n_folds)
# Lambdas
lambdas <-  c(4.25, seq(4.5, 4.7, by = .1), 5)
# Matrix to store RMSEs
rmse_matrix <- matrix(NA, nrow = length(lambdas), ncol = n_folds)

cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)

for (i in seq_along(lambdas)) {
  for (j in seq_along(folds$splits)) {
    train_data <- analysis(folds$splits[[j]])
    test_data  <- assessment(folds$splits[[j]])
    rmse_matrix[i, j] <- RMSEs(train_data, test_data, lambdas[i])
  }
}
stopCluster(cl)
# Mean RMSEs for each lambda
mean_rmse <- rowMeans(rmse_matrix)

# Lambda that minimizes RMSE
optimal_lambda <- lambdas[which.min(mean_rmse)]
```

Since RMSE is a quadratic function, any point where its curve changes direction, known as an inflection point, can be used to identify the minimum RMSE. In Figure 5 below, the combinations of lambdas and RMSE are depicted, with the minimum $\lambda$ = `r optimal_lambda` value indicated by a dot.

```{r, eval = T}
# Plot for lambdas
lambdas_df <- tibble(lambdas, mean_rmse)
lambdas_df |> 
ggplot(aes(lambdas, mean_rmse)) +
  geom_line() +
  geom_point(aes(lambdas, mean_rmse), 
             data = subset(lambdas_df, 
                           lambdas == optimal_lambda)) +
  labs(title = 'Figure 5: RMSE vs. lambdas',
       y = 'RMSEs')
```

## Regularization

In the context of movie ratings prediction, L2 regularization serves as a tool to mitigate overfitting and enhance the generalization capability of results. L2 regularization, also known as Ridge regression, introduces a penalty term to the model's cost function, discouraging excessively large coefficients for predictors. This penalty term is proportional to the square of the magnitude of the coefficients, effectively shrinking them towards zero without eliminating them entirely.


```{r regularization}
# Set the regularization parameter
lambda = optimal_lambda
# Get the global average rating
mu <- mean(training_set$rating, na.rm = T)
# Movie bias
b_movie <- training_set |> 
  group_by(movieId) |> 
  summarize(b_movie = sum(rating - mu) / (n() + lambda))
# User bias
b_user <- training_set |> 
  left_join(b_movie, by = 'movieId') |> 
  group_by(userId) |> 
  summarize(b_user = sum(rating - mu - b_movie) / (n() + lambda))
# Movie genre bias
b_genres <- training_set |> 
  left_join(b_movie, by = 'movieId') |> 
  left_join(b_user,  by = 'userId') |> 
  group_by(genres) |>
  summarize(b_genres = sum(rating - mu - b_movie - b_user) / (n() + lambda))
# Year bias
b_year <- training_set |> 
  left_join(b_movie,   by = 'movieId') |>
  left_join(b_user,    by = 'userId') |>
  left_join(b_genres,  by = 'genres') |>
  group_by(year_reviewed) |> 
  summarize(b_year = 
              sum(rating - mu - b_movie - b_user - b_genres) / (n() + lambda))

# Adding year_reviewed to final_holdout_test
final_holdout_test <- final_holdout_test |> 
  mutate(year_reviewed = year(as_datetime(timestamp)))
# Predicted ratings as sums of mu and all biases
predicted_ratings <- final_holdout_test |> 
  left_join(b_movie,  by = 'movieId') |> 
  left_join(b_user,   by = 'userId') |> 
  left_join(b_genres, by = 'genres') |> 
  left_join(b_year,   by = 'year_reviewed')
predicted_ratings <- predicted_ratings |> 
mutate(pred = mu + b_movie + b_user + b_genres + b_year)

RMSE_reg <- RMSE(final_holdout_test$rating, predicted_ratings$pred)
```

The RMSE of regularized model, `r RMSE_reg`, indicates a marginal increase in model performace.
 
```{r, out_of_range_ratings, eval = T}
# Assign min and max to less than .5 and greater than 5 predictions
constrained_predicted_ratings <- predicted_ratings |> 
  mutate(pred = case_when(pred < .5 ~ .5,
                          pred >  5 ~  5,
                      .default = pred))

RMSE_constrained <- RMSE(constrained_predicted_ratings$pred, final_holdout_test$rating)

points_df <- tibble(x = seq(.5, 5, .5),
                    y = seq(.5, 5, .5))

results <- tibble(actual = final_holdout_test$rating, predicted = 
                    constrained_predicted_ratings$pred, half = final_holdout_test$half_rating)

plot_dist <- results |> 
  ggplot(aes(x = predicted, 
             y = actual, 
         group = actual, 
          fill = half)) + 
  geom_density_ridges(stat = "binline", binwidth = 0.07, 
                      scale = 0.9, color = 'white', alpha = 1/2) +
  geom_abline(linewidth = .2) +
  labs(title = 'Figure 6: Comparing actual and predicted ratings',
       y = 'Actual ratings',
       x = 'Predicted ratings') + 
  theme(legend.position = 'none')
```
Finally, some of the predictions are inadmissible, as they fall outside the possible rating limits. The acceptable range for ratings is between 0.5 and 5, inclusive (\(0.5 \leq \text{rating} \leq 5\)). Predictions that do not meet this criterion were excluded from the analysis. After removing these extreme ratings, RMSE was recalculated, resulting in an RMSE of `r RMSE_constrained`.

Table 7 provides a comprehensive overview of the RMSEs values for each method. Additionally, it includes a column (Growth rate) denoting the improvement in percentage terms from one model relative to the previous, offering some insights into the efficacy of each approach relative to others.

```{r}
# Final table - all RMSEs
tribble(~Model, ~RMSE, 
        'Regularized', RMSE_reg,
        "Constrained ($0.5 \\leq \\text{rating} \\leq 5.0$)", RMSE_constrained) |> 
  rbind(interm_table) |>
  arrange(desc(RMSE)) |>
  mutate(`Growth rate` = (RMSE - lag(RMSE)) / lag(RMSE) * 100) |> 
  mutate(`Growth rate`  = round(`Growth rate`, 3)) |> 
  arrange(desc(RMSE)) |>
  kbl(format = 'latex', booktabs = TRUE, escape = FALSE, 
    linesep = '', 
    caption = 'RMSE') |>
  kable_styling(latex_options = c('hold_position'))
```

The performance of various models in predicting movie ratings shows significant improvements at each stage of complexity. Starting from the simple global average model with an RMSE of `r mu_rmse`, the introduction of movie-specific effects reduces the RMSE by approximately 11 percent, marking a substantial enhancement. Adding user-specific effects further decreases the RMSE to`r RMSE_user`, with an 8 percent improvement, indicating the critical role of user preferences in predictions. Incorporating genre effects leads to a marginal improvement with a 0.05 percent reduction in RMSE, while adding year effects shows an almost negligible enhancement. Regularization slightly improves the model performance, reducing the RMSE by 0.07 percent. Finally, constraining ratings within the range of 0.5 to 5.0 results in a very minor improvement of about 0.01 percent. Overall, the most notable improvements are observed with the inclusion of movie and user effects, while subsequent refinements provide diminishing returns.

Figure 6 below compares the distribution of predicted ratings along the horizontal axis and the actual ratings along the vertical axis. These distributions are color-coded, with integer ratings depicted in orange and fractional ratings in green. Notably, these distributions show (slightly) reduced variability towards the higher end of the rating scale (5). However, the most significant observation in this figure pertains to the systematic bias apparent in these distributions when compared to the diagonal line, which represents the set of values where predicted and actual ratings coincide (isoline). Notably, the modal value (peak of the distribution) for a rating of 3.5 appears unbiased, as the diagonal line closely aligns with the center of the corresponding distribution. However, the predicted ratings tend to consistently underestimate the actual ratings for values higher than 3.5, as evidenced by the distribution being mostly located to the left of the diagonal line. Conversely, for ratings between 0.5 and 3, most of the distribution tends to be located to the right of the diagonal line, suggesting an overestimation. This indicates that the model might be underfitting and producing biased results due to its simplicity.
```{r}
plot_dist
```

# IV. Conclusion

The model presented uses biases associated with movies, users, genres, and years to address the challenge of excessive predictors in regression analysis, often rendering traditional methods impractical with available computing resources. Through regularization, it seeks to strike a balance between complexity and predictive accuracy, optimizing the regularization parameter ($\lambda$) through a tuning process to minimize the loss function (RMSE). However, as observed previously, this approach also displays signs of underfitting, an expected outcome given its simplicity.

Exploring alternative methodologies to tackle computational constraints could enhance the model's effectiveness. This could involve utilizing distributed computing frameworks or implementing algorithmic optimizations tailored for large-scale datasets. Furthermore, integrating feature engineering techniques may enrich the model's predictive capabilities by extracting more informative features from the data.

Despite its limitations, this project presents an interesting approach to predictive modeling, showcasing the integration of biases and regularization techniques to overcome challenges in regression analysis. Future extensions could focus on refining computational efficiency and feature engineering methodologies, ultimately enhancing the model's performance.

## References

Irizarry, A. Rafael (2020). *Introduction to Data Science*. CRC Press.

Maxwell Harper, F. and Joseph A. Konstan (2015). *The MovieLens datasets: History and context.* ACM Trans. Interact. Intell. Syst. 5, 4. DOI: http://dx.doi.org/10.1145/2827872

Wainer, Howard (2007). *The Most Dangerous Equation.* American Scientist. Vol. 95, pp. 249 - 256.

## Session Info

```{r}
pander::pander(sessionInfo()) 
```

